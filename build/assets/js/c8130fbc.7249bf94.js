"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[405],{7541:function(e,n,t){t.r(n),t.d(n,{assets:function(){return l},contentTitle:function(){return c},default:function(){return d},frontMatter:function(){return o},metadata:function(){return i},toc:function(){return a}});var i=JSON.parse('{"id":"module-3-ai-perception/07-perception-pipelines","title":"Chapter 7 - Complete Perception Pipelines","description":"Introduction","source":"@site/docs/03-module-3-ai-perception/02-perception-pipelines.md","sourceDirName":"03-module-3-ai-perception","slug":"/module-3-ai-perception/07-perception-pipelines","permalink":"/native-book/module-3-ai-perception/07-perception-pipelines","draft":false,"unlisted":false,"editUrl":"https://github.com/native-book/native-book/tree/main/docs/03-module-3-ai-perception/02-perception-pipelines.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"07-perception-pipelines","title":"Chapter 7 - Complete Perception Pipelines","sidebar_label":"Chapter 7: Perception Pipelines"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 6: Isaac Basics","permalink":"/native-book/module-3-ai-perception/06-isaac-fundamentals"},"next":{"title":"Chapter 8: LLM Integration","permalink":"/native-book/module-4-voice-control/08-llm-integration"}}'),s=t(4848),r=t(8453);const o={id:"07-perception-pipelines",title:"Chapter 7 - Complete Perception Pipelines",sidebar_label:"Chapter 7: Perception Pipelines"},c="Chapter 7: Complete Perception Pipelines",l={},a=[{value:"Introduction",id:"introduction",level:2},{value:"The Full Pipeline Problem",id:"the-full-pipeline-problem",level:3},{value:"Pipeline Architecture",id:"pipeline-architecture",level:3},{value:"Multi-Stage Detection Pipeline",id:"multi-stage-detection-pipeline",level:2},{value:"Object Detection + Tracking",id:"object-detection--tracking",level:3},{value:"Expected Output",id:"expected-output",level:3},{value:"Real-World Pipeline: Humanoid Pick-and-Place",id:"real-world-pipeline-humanoid-pick-and-place",level:2},{value:"Perception + Control Integration",id:"perception--control-integration",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU vs CPU Trade-offs",id:"gpu-vs-cpu-trade-offs",level:3},{value:"Optimization Strategies",id:"optimization-strategies",level:3},{value:"Summary",id:"summary",level:2}];function p(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-7-complete-perception-pipelines",children:"Chapter 7: Complete Perception Pipelines"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["In Chapter 6, you learned about GPU acceleration and individual AI tasks (detection, pose estimation). But real robots need ",(0,s.jsx)(n.strong,{children:"complete pipelines"})," that combine multiple perception tasks into a coherent understanding of the world."]}),"\n",(0,s.jsx)(n.p,{children:"This chapter shows you how to build production-grade perception systems."}),"\n",(0,s.jsx)(n.h3,{id:"the-full-pipeline-problem",children:"The Full Pipeline Problem"}),"\n",(0,s.jsx)(n.p,{children:"Imagine a humanoid robot picking up an object:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Step 1: Detect objects in scene\nStep 2: Identify which is the target object\nStep 3: Estimate its 3D position\nStep 4: Plan arm motion to reach it\nStep 5: Execute motion while tracking object\nStep 6: Detect grasp success\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Each step is a separate AI task. ",(0,s.jsx)(n.strong,{children:"Combining them efficiently"})," while maintaining real-time performance is the challenge."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Source"}),": NVIDIA Isaac ROS Documentation. (2024). Retrieved from ",(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"https://github.com/NVIDIA-ISAAC-ROS"})]}),"\n",(0,s.jsx)(n.h3,{id:"pipeline-architecture",children:"Pipeline Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A complete perception pipeline:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Camera Input]\n     \u2193\n[Preprocessing]\n  - Color correction\n  - Resizing\n  - Normalization\n     \u2193\n[Detection] (GPU)\n  - YOLO inference\n  - Filter by class\n  - NMS (remove duplicates)\n     \u2193\n[Tracking] (GPU)\n  - Associate with previous frame\n  - Maintain object IDs\n  - Estimate velocity\n     \u2193\n[Pose Estimation] (GPU)\n  - 3D position from 2D detection + depth\n  - Orientation estimation\n  - Confidence scoring\n     \u2193\n[Decision Making] (CPU/GPU)\n  - Choose best target\n  - Plan actions\n  - Estimate feasibility\n     \u2193\n[Output Topics]\n  - /detected_objects\n  - /object_poses\n  - /recommended_action\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Each stage processes data and passes results to the next. All GPU stages run ",(0,s.jsx)(n.strong,{children:"in parallel"}),", achieving real-time throughput."]}),"\n",(0,s.jsx)(n.h2,{id:"multi-stage-detection-pipeline",children:"Multi-Stage Detection Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"object-detection--tracking",children:"Object Detection + Tracking"}),"\n",(0,s.jsx)(n.p,{children:"A practical pipeline combining detection and tracking:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nComplete Perception Pipeline\nCombines detection, tracking, and pose estimation\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Int32\nfrom cv_bridge import CvBridge\nimport time\n\n\nclass PerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('perception_pipeline')\n\n        # Input: camera images\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.process_image,\n            10)\n\n        # Output 1: Detected objects\n        self.detection_pub = self.create_publisher(\n            PoseStamped,\n            '/detected_objects',\n            10)\n\n        # Output 2: Tracked object IDs\n        self.tracking_pub = self.create_publisher(\n            Int32,\n            '/tracked_id',\n            10)\n\n        # Performance tracking\n        self.frame_count = 0\n        self.stage_times = {\n            'detection': 0,\n            'tracking': 0,\n            'pose': 0\n        }\n\n        self.bridge = CvBridge()\n        self.object_id = 0\n        self.fps_start = time.time()\n\n        self.get_logger().info('Perception pipeline started')\n\n    def process_image(self, msg):\n        \"\"\"Process image through complete perception pipeline\"\"\"\n        # Stage 1: Preprocessing (measure timing)\n        t0 = time.time()\n\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        except Exception as e:\n            self.get_logger().error(f'Image conversion failed: {e}')\n            return\n\n        t_preprocess = time.time() - t0\n\n        # Stage 2: Detection (GPU-accelerated in real Isaac)\n        t0 = time.time()\n        detections = self.detect_objects(cv_image)\n        t_detect = time.time() - t0\n\n        # Stage 3: Tracking (GPU-accelerated in real Isaac)\n        t0 = time.time()\n        tracked_objects = self.track_objects(detections)\n        t_track = time.time() - t0\n\n        # Stage 4: Pose estimation (GPU-accelerated)\n        t0 = time.time()\n        poses = self.estimate_poses(tracked_objects)\n        t_pose = time.time() - t0\n\n        # Stage 5: Decision making (CPU)\n        best_target = self.select_target(poses)\n\n        # Publish results\n        self.detection_pub.publish(best_target)\n        self.tracking_pub.publish(Int32(data=self.object_id))\n\n        # Log pipeline timing\n        self.frame_count += 1\n        elapsed = time.time() - self.fps_start\n        if elapsed >= 1.0:\n            fps = self.frame_count / elapsed\n            total_time = t_preprocess + t_detect + t_track + t_pose\n            self.get_logger().info(\n                f'Pipeline: {fps:.1f}fps, {total_time*1000:.1f}ms '\n                f'(detect:{t_detect*1000:.1f}ms, track:{t_track*1000:.1f}ms)')\n            self.frame_count = 0\n            self.fps_start = time.time()\n\n    def detect_objects(self, image):\n        \"\"\"Stage 1: GPU-accelerated object detection (YOLO)\"\"\"\n        # In real Isaac, this runs YOLO on GPU\n        # Returns: list of bounding boxes + confidence scores\n        return [\n            {'x': image.shape[1]//2, 'y': image.shape[0]//2,\n             'conf': 0.95, 'class': 'cup'}\n        ]\n\n    def track_objects(self, detections):\n        \"\"\"Stage 2: GPU-accelerated tracking\"\"\"\n        # Associates detections with previous frame\n        # Maintains object IDs across frames\n        self.object_id = 1\n        for det in detections:\n            det['id'] = self.object_id\n        return detections\n\n    def estimate_poses(self, tracked_objects):\n        \"\"\"Stage 3: GPU-accelerated 3D pose estimation\"\"\"\n        # Converts 2D detection + depth \u2192 3D position + orientation\n        poses = []\n        for obj in tracked_objects:\n            pose = PoseStamped()\n            pose.header.frame_id = 'camera'\n            pose.pose.position.x = 0.5\n            pose.pose.position.y = 0.0\n            pose.pose.position.z = 0.0\n            pose.pose.orientation.w = 1.0\n            poses.append(pose)\n        return poses\n\n    def select_target(self, poses):\n        \"\"\"Stage 4: Decision making - choose best target\"\"\"\n        # Select closest, most confident, most reachable object\n        if poses:\n            return poses[0]\n        return PoseStamped()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    pipeline = PerceptionPipeline()\n    rclpy.spin(pipeline)\n    pipeline.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What's happening"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stage 1 - Detection"}),": Run YOLO on GPU (20-30 fps)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stage 2 - Tracking"}),": Associate objects across frames (GPU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stage 3 - Pose"}),": Estimate 3D position + orientation (GPU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stage 4 - Decision"}),": Choose action based on poses (CPU)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Publish best target to ROS 2 topics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor"}),": Track timing of each stage"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"expected-output",children:"Expected Output"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Terminal"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[perception_pipeline] Perception pipeline started\n[perception_pipeline] Pipeline: 28.5fps, 35.2ms (detect:15.3ms, track:8.9ms)\n[perception_pipeline] Pipeline: 29.2fps, 34.1ms (detect:15.1ms, track:8.8ms)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Maintaining 28+ fps = real-time perception \u2705"}),"\n",(0,s.jsx)(n.h2,{id:"real-world-pipeline-humanoid-pick-and-place",children:"Real-World Pipeline: Humanoid Pick-and-Place"}),"\n",(0,s.jsx)(n.p,{children:"A complete example for a humanoid robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Input: RGB camera + depth camera\n\nPipeline:\n1. Detect objects \u2192 list of candidates\n2. Filter by size, color \u2192 valid targets\n3. Track across 5 frames \u2192 reduce false positives\n4. Estimate 3D pose \u2192 transform to robot frame\n5. Check reachability \u2192 can arm reach it?\n6. Plan grasp \u2192 generate gripper motion\n7. Execute \u2192 send command to arm\n8. Monitor \u2192 track success/failure\n\nOutput: Robot picks up object\n"})}),"\n",(0,s.jsx)(n.p,{children:"Each stage runs optimally:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU stages"})," (detect, track, pose): Run in parallel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU stages"})," (filter, check, plan): Run sequentially"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Overall"}),": 20-30 fps end-to-end"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"perception--control-integration",children:"Perception + Control Integration"}),"\n",(0,s.jsx)(n.p,{children:"The beauty of perception pipelines: they integrate seamlessly with ROS 2 control nodes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[Perception Node] \u2192  /object_poses topic  \u2192 [Control Node]\n      \u2191                                            \u2193\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 /joint_states topic \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Control node"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Subscribes to ",(0,s.jsx)(n.code,{children:"/object_poses"})]}),"\n",(0,s.jsxs)(n.li,{children:["Reads current ",(0,s.jsx)(n.code,{children:"/joint_states"})]}),"\n",(0,s.jsx)(n.li,{children:"Plans and executes motion"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Same code works"}),": In simulation or with real perception"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-vs-cpu-trade-offs",children:"GPU vs CPU Trade-offs"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Operation"}),(0,s.jsx)(n.th,{children:"CPU Time"}),(0,s.jsx)(n.th,{children:"GPU Time"}),(0,s.jsx)(n.th,{children:"Speedup"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Detect 1920\xd71080"}),(0,s.jsx)(n.td,{children:"100ms"}),(0,s.jsx)(n.td,{children:"5ms"}),(0,s.jsx)(n.td,{children:"20\xd7"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Track objects"}),(0,s.jsx)(n.td,{children:"50ms"}),(0,s.jsx)(n.td,{children:"10ms"}),(0,s.jsx)(n.td,{children:"5\xd7"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Pose estimation"}),(0,s.jsx)(n.td,{children:"30ms"}),(0,s.jsx)(n.td,{children:"2ms"}),(0,s.jsx)(n.td,{children:"15\xd7"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total pipeline"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"180ms"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"17ms"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"10\xd7"})})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"GPU acceleration enables real-time performance."}),"\n",(0,s.jsx)(n.h3,{id:"optimization-strategies",children:"Optimization Strategies"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch processing"}),": Process multiple frames in parallel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model optimization"}),": Use quantized models for faster inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resolution tuning"}),": 720p for speed, 1080p for accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Store results between frames when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Parallel execution"}),": Run detection while tracking last frame"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complete pipelines"}),": Combine multiple perception tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-stage processing"}),": Detection \u2192 Tracking \u2192 Pose \u2192 Decision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU parallelism"}),": All GPU stages run in parallel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time performance"}),": Achieve 20-30 fps on complex pipelines"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 integration"}),": Publish results for control nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance monitoring"}),": Track timing of each stage"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaway"}),": GPU-accelerated pipelines enable robots to understand their world in real-time and act intelligently."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"What this enables"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Autonomous navigation (perceive obstacles, plan path)"}),"\n",(0,s.jsx)(n.li,{children:"Object manipulation (detect, reach, grasp, place)"}),"\n",(0,s.jsx)(n.li,{children:"Human interaction (recognize, respond, collaborate)"}),"\n",(0,s.jsx)(n.li,{children:"Scene understanding (segment, classify, estimate properties)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"References"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["NVIDIA Isaac ROS Documentation. (2024). Retrieved from ",(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"https://github.com/NVIDIA-ISAAC-ROS"})]}),"\n",(0,s.jsxs)(n.li,{children:["Real-Time Perception Systems. (2024). ",(0,s.jsx)(n.em,{children:"Robotics Research"}),". Retrieved from ",(0,s.jsx)(n.a,{href:"https://developer.nvidia.com/isaac",children:"https://developer.nvidia.com/isaac"})]}),"\n",(0,s.jsxs)(n.li,{children:["YOLOv8 Real-Time Performance. (2024). Retrieved from ",(0,s.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"https://docs.ultralytics.com/"})]}),"\n",(0,s.jsxs)(n.li,{children:["ROS 2 Sensor Processing. (2024). Retrieved from ",(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials/Vision-OpenCV.html",children:"https://docs.ros.org/en/humble/Tutorials/Vision-OpenCV.html"})]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:function(e,n,t){t.d(n,{R:function(){return o},x:function(){return c}});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);