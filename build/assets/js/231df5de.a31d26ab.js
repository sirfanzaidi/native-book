"use strict";(self.webpackChunkphysical_ai_humanoid_robotics_book=self.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[895],{4082:function(e,n,t){t.r(n),t.d(n,{assets:function(){return a},contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return o},metadata:function(){return s},toc:function(){return c}});var s=JSON.parse('{"id":"capstone-project/10-capstone-project","title":"Chapter 10 - Capstone: Autonomous Furniture Assembly Robot","description":"Introduction","source":"@site/docs/05-capstone-project/01-capstone-overview.md","sourceDirName":"05-capstone-project","slug":"/capstone-project/10-capstone-project","permalink":"/native-book/capstone-project/10-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/native-book/native-book/tree/main/docs/05-capstone-project/01-capstone-overview.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"id":"10-capstone-project","title":"Chapter 10 - Capstone: Autonomous Furniture Assembly Robot","sidebar_label":"Chapter 10: Capstone Project"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9: Vision-Language-Action","permalink":"/native-book/module-4-voice-control/09-vision-language-action"},"next":{"title":"Chapter 11: End-to-End Implementation","permalink":"/native-book/capstone-project/11-capstone-implementation"}}'),i=t(4848),r=t(8453);const o={id:"10-capstone-project",title:"Chapter 10 - Capstone: Autonomous Furniture Assembly Robot",sidebar_label:"Chapter 10: Capstone Project"},l="Chapter 10: Capstone Project - Autonomous Furniture Assembly Robot",a={},c=[{value:"Introduction",id:"introduction",level:2},{value:"The Complete Architecture",id:"the-complete-architecture",level:3},{value:"The Task: Furniture Assembly",id:"the-task-furniture-assembly",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Why This Capstone",id:"why-this-capstone",level:3},{value:"System Components",id:"system-components",level:2},{value:"1. Task Planner (LLM-Based)",id:"1-task-planner-llm-based",level:3},{value:"2. Vision-Language-Action Executor",id:"2-vision-language-action-executor",level:3},{value:"3. Perception Pipeline",id:"3-perception-pipeline",level:3},{value:"4. ROS 2 Command Interface",id:"4-ros-2-command-interface",level:3},{value:"Complete Example: Multi-Step Assembly",id:"complete-example-multi-step-assembly",level:2},{value:"Expected Output",id:"expected-output",level:3},{value:"Why This Capstone Matters",id:"why-this-capstone-matters",level:2},{value:"What You&#39;ve Achieved",id:"what-youve-achieved",level:2},{value:"Next Steps (Beyond This Book)",id:"next-steps-beyond-this-book",level:2},{value:"For Practitioners",id:"for-practitioners",level:3},{value:"For Researchers",id:"for-researchers",level:3},{value:"For Educators",id:"for-educators",level:3},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-10-capstone-project---autonomous-furniture-assembly-robot",children:"Chapter 10: Capstone Project - Autonomous Furniture Assembly Robot"})}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsxs)(n.p,{children:["This capstone chapter brings together ",(0,i.jsx)(n.strong,{children:"everything you've learned"})," across all 4 modules:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 1"}),": ROS 2 communication (nodes, topics, services)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 2"}),": Simulation environment (Gazebo physics)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 3"}),": AI perception (GPU-accelerated detection)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Module 4"}),": Autonomy (LLM reasoning + vision-language-action)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["You'll build a ",(0,i.jsx)(n.strong,{children:"complete autonomous system"})," where a humanoid robot assembles IKEA-style furniture using natural language commands, vision perception, and real-time decision making."]}),"\n",(0,i.jsx)(n.h3,{id:"the-complete-architecture",children:"The Complete Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         AUTONOMOUS FURNITURE ASSEMBLY ROBOT SYSTEM                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  [Human Command] \u2192 "Assemble the bookshelf"                        \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u2193                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 LAYER 4: Autonomy (Module 4)                                 \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 LLM: Decompose task into steps                            \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 VLA: Understand intent + perceive objects + act           \u2502  \u2502\n\u2502  \u2502 \u2514\u2500 State: Track assembly progress                            \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u2193                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 LAYER 3: Perception (Module 3)                               \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Vision: Detect furniture parts (GPU-accelerated)          \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Tracking: Follow part movement in 3D                      \u2502  \u2502\n\u2502  \u2502 \u2514\u2500 Pose Est: Estimate part orientation for grasping          \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u2193                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 LAYER 2: Simulation (Module 2)                               \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Physics: Gazebo simulates gravity, collisions, friction   \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Sensors: Camera, IMU, joint feedback                      \u2502  \u2502\n\u2502  \u2502 \u2514\u2500 Control: Joint commands \u2192 physics engine \u2192 new state      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u2193                                                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 LAYER 1: Robot Control (Module 1)                            \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Publisher: Send joint commands via topics                 \u2502  \u2502\n\u2502  \u2502 \u251c\u2500 Subscriber: Receive sensor feedback                       \u2502  \u2502\n\u2502  \u2502 \u2514\u2500 Service: Query part locations, verify assembly steps      \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                                           \u2502\n\u2502         \u2193                                                           \u2502\n\u2502  [Furniture Assembly Complete] \u2713                                   \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,i.jsx)(n.h2,{id:"the-task-furniture-assembly",children:"The Task: Furniture Assembly"}),"\n",(0,i.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'Human: "Assemble the IKEA LACK bookshelf. Parts are in boxes A and B.\n        When finished, place it against the wall."\n\nRobot executes:\n1. [Language] Understand: "assemble LACK" \u2192 recognize assembly steps\n2. [Vision] Perceive: Detect all parts in scene \u2192 identify each component\n3. [Plan] Generate: Which part first? How to orient? Where to place?\n4. [Act] Execute: Grasp \u2192 position \u2192 insert \u2192 verify \u2192 repeat\n5. [Feedback] Monitor: Did peg insert correctly? Is shelf stable?\n6. [Adapt] Recover: Part misaligned? Retract and reposition\n7. [Report] Communicate: "Step 1/4: Left panel attached"\n\nResult: Assembled bookshelf ready for use \u2713\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Source"}),": NVIDIA. (2024). ",(0,i.jsx)(n.em,{children:"Autonomous Manipulation with Vision-Language Models"}),". Retrieved from ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/chatgpt-robotics",children:"https://developer.nvidia.com/blog/chatgpt-robotics"})]}),"\n",(0,i.jsx)(n.h3,{id:"why-this-capstone",children:"Why This Capstone"}),"\n",(0,i.jsx)(n.p,{children:"This project exercises:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"All ROS 2 patterns"}),": Publishers, subscribers, services, actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physics simulation"}),": Realistic grasp stability, assembly tolerance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time perception"}),": Detecting deformable parts, tracking motion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language understanding"}),": Parsing assembly instructions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Motion planning"}),": Collision avoidance, inverse kinematics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback loops"}),": Verify each step, adapt on failure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error recovery"}),": Handle part misalignment, retry logic"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"system-components",children:"System Components"}),"\n",(0,i.jsx)(n.h3,{id:"1-task-planner-llm-based",children:"1. Task Planner (LLM-Based)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'System Prompt: "You are a robot assembly expert.\nGiven: furniture name, available parts, assembly steps.\nGenerate: ordered sequence of actions.\nFormat: JSON with step number, action, constraints."\n\nUser Input: "Assemble IKEA LACK bookshelf with parts in Box A and B"\n\nLLM Output:\n{\n  "task": "assemble_bookshelf",\n  "steps": [\n    {"step": 1, "action": "attach_left_panel", "priority": "high"},\n    {"step": 2, "action": "attach_right_panel", "priority": "high"},\n    {"step": 3, "action": "insert_shelves", "priority": "medium"},\n    {"step": 4, "action": "verify_stability", "priority": "critical"}\n  ]\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-vision-language-action-executor",children:"2. Vision-Language-Action Executor"}),"\n",(0,i.jsx)(n.p,{children:"For each step:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understand"}),": What part am I working with?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perceive"}),": Where is it? What's its orientation?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ground"}),': Match language to vision ("left panel" \u2192 detected panel at pose)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": Motion to grasp and position"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execute"}),": Move robot to grasp point"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Verify"}),": Did it work? Can I sense it?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adapt"}),": If failed, retry with different approach"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-perception-pipeline",children:"3. Perception Pipeline"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Camera Image \u2192 Detection \u2192 Tracking \u2192 Pose Estimation \u2192 Action\n                \u2193           \u2193          \u2193\n           Part types   Continuous  6-DOF pose\n           (panel,      motion      for gripper\n            shelf,      across      alignment\n            peg)        frames\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-ros-2-command-interface",children:"4. ROS 2 Command Interface"}),"\n",(0,i.jsx)(n.p,{children:"All communication via standard ROS 2:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Topics"}),": Joint commands, sensor feedback, perception results"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Services"}),": Query part locations, verify assembly state"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actions"}),": Multi-step movements with feedback"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"complete-example-multi-step-assembly",children:"Complete Example: Multi-Step Assembly"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nAutonomous Furniture Assembly System\nIntegrates all 4 modules: ROS 2, Gazebo, Isaac perception, LLM reasoning\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped, Twist\nimport time\n\n\nclass FurnitureAssemblyRobot(Node):\n    def __init__(self):\n        super().__init__('furniture_assembly_robot')\n\n        # Perception inputs (Module 3)\n        self.camera_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.on_camera_frame,\n            10)\n\n        self.detection_sub = self.create_subscription(\n            PoseStamped,\n            '/isaac/detections',\n            self.on_detection,\n            10)\n\n        # Language input (Module 4)\n        self.command_sub = self.create_subscription(\n            String,\n            '/assembly_command',\n            self.on_assembly_command,\n            10)\n\n        # Robot control outputs (Module 1)\n        self.joint_pub = self.create_publisher(\n            Twist,\n            '/joint_commands',\n            10)\n\n        self.status_pub = self.create_publisher(\n            String,\n            '/assembly_status',\n            10)\n\n        # State\n        self.latest_image = None\n        self.latest_detections = {}\n        self.assembly_plan = []\n        self.current_step = 0\n\n        self.get_logger().info('Furniture Assembly Robot initialized')\n        self.get_logger().info('Ready to receive: /assembly_command')\n\n    def on_camera_frame(self, msg):\n        \"\"\"Update latest camera frame (Module 3)\"\"\"\n        self.latest_image = msg\n\n    def on_detection(self, msg):\n        \"\"\"Update detected part positions (Module 3)\"\"\"\n        # Store detection with timestamp for tracking\n        part_id = f\"part_{int(msg.header.stamp.sec)}\"\n        self.latest_detections[part_id] = {\n            'pose': msg.pose,\n            'timestamp': msg.header.stamp\n        }\n\n    def on_assembly_command(self, msg):\n        \"\"\"Receive assembly task command (Module 4)\"\"\"\n        command = msg.data\n        self.get_logger().info(f'\ud83e\udd16 Assembly Command: {command}')\n\n        # Step 1: Parse with LLM (Module 4)\n        self.assembly_plan = self.llm_parse_task(command)\n        self.get_logger().info(f'\ud83d\udccb Generated {len(self.assembly_plan)} steps')\n\n        # Step 2: Execute plan with feedback loop\n        self.execute_assembly_plan()\n\n    def llm_parse_task(self, command):\n        \"\"\"\n        Use LLM to decompose assembly task\n        In production: call OpenAI API\n        \"\"\"\n        # Simulated task decomposition\n        if 'bookshelf' in command.lower():\n            return [\n                {'step': 1, 'action': 'pick_left_panel', 'target': 'panel_left'},\n                {'step': 2, 'action': 'position_vertically', 'height': 1.2},\n                {'step': 3, 'action': 'place_on_base', 'orientation': 'vertical'},\n                {'step': 4, 'action': 'pick_right_panel', 'target': 'panel_right'},\n                {'step': 5, 'action': 'position_vertically', 'height': 1.2},\n                {'step': 6, 'action': 'attach_to_left_panel', 'joint_type': 'peg'},\n                {'step': 7, 'action': 'insert_shelves', 'count': 4},\n                {'step': 8, 'action': 'verify_stability', 'check': 'all_joints'}\n            ]\n        return []\n\n    def execute_assembly_plan(self):\n        \"\"\"Execute multi-step assembly with feedback (Module 4 VLA)\"\"\"\n        for step_info in self.assembly_plan:\n            step_num = step_info['step']\n            action = step_info['action']\n\n            self.get_logger().info(f'Step {step_num}/{len(self.assembly_plan)}: {action}')\n            self.publish_status(f'Step {step_num}/{len(self.assembly_plan)}: {action}')\n\n            # Execute VLA for this step\n            success = self.execute_vla_step(step_info)\n\n            if not success:\n                # Error recovery: retry with different approach\n                self.get_logger().warn(f'Step {step_num} failed, retrying...')\n                self.publish_status(f'Retrying step {step_num}')\n                success = self.execute_vla_step(step_info, retry=True)\n\n            if not success:\n                self.get_logger().error(f'Step {step_num} failed after retry')\n                self.publish_status(f'ERROR: Step {step_num} failed')\n                return\n\n            time.sleep(1)  # Brief pause between steps\n\n        # Assembly complete\n        self.get_logger().info('\u2705 Assembly complete!')\n        self.publish_status('Assembly complete - furniture ready')\n\n    def execute_vla_step(self, step_info, retry=False):\n        \"\"\"\n        Execute Vision-Language-Action for single assembly step\n        Uses all 4 modules:\n        1. Language understanding (Module 4) - understand action\n        2. Vision perception (Module 3) - detect parts\n        3. Gazebo simulation (Module 2) - predict outcomes\n        4. ROS 2 control (Module 1) - execute movements\n        \"\"\"\n        action = step_info['action']\n        target = step_info.get('target')\n\n        # Phase 1: Language understanding\n        intent = self.understand_action(action)\n\n        # Phase 2: Vision perception\n        parts = self.detect_relevant_parts(intent)\n        if not parts:\n            self.get_logger().warn(f'Could not detect {target}')\n            return False\n\n        # Phase 3: Grounding - link language to vision\n        target_part = self.ground_to_detected_part(intent, parts)\n        if not target_part:\n            return False\n\n        # Phase 4: Motion planning\n        trajectory = self.plan_motion(action, target_part, retry)\n        if not trajectory:\n            return False\n\n        # Phase 5: Execution\n        success = self.execute_trajectory(trajectory)\n\n        # Phase 6: Verification\n        if success:\n            verified = self.verify_step_complete(action)\n            return verified\n\n        return False\n\n    def understand_action(self, action):\n        \"\"\"Parse action string to intent\"\"\"\n        intents = {\n            'pick': 'grasp_object',\n            'position': 'move_to_location',\n            'attach': 'join_parts',\n            'insert': 'slide_into_slot',\n            'verify': 'check_state'\n        }\n        for key, intent in intents.items():\n            if key in action:\n                return intent\n        return 'unknown'\n\n    def detect_relevant_parts(self, intent):\n        \"\"\"Use Module 3 perception to find parts\"\"\"\n        # In real system: Run Isaac perception pipeline\n        # Returns: list of detected parts with poses\n        return list(self.latest_detections.values())\n\n    def ground_to_detected_part(self, intent, parts):\n        \"\"\"Ground language concept to detected object\"\"\"\n        if parts:\n            return parts[0]  # In real system: semantic matching\n        return None\n\n    def plan_motion(self, action, target_part, retry=False):\n        \"\"\"Plan collision-free motion using Module 2 simulation\"\"\"\n        # In real system: Use Gazebo MoveIt for planning\n        # Returns: sequence of joint states\n        return [{'joint_1': 0.5, 'joint_2': 0.3}]  # Simulated trajectory\n\n    def execute_trajectory(self, trajectory):\n        \"\"\"Execute via ROS 2 publishers (Module 1)\"\"\"\n        for waypoint in trajectory:\n            # Convert to Twist message and publish\n            cmd = Twist()\n            cmd.linear.x = waypoint.get('joint_1', 0.0)\n            cmd.angular.z = waypoint.get('joint_2', 0.0)\n            self.joint_pub.publish(cmd)\n            time.sleep(0.1)\n        return True\n\n    def verify_step_complete(self, action):\n        \"\"\"Verify step succeeded via sensor feedback\"\"\"\n        # Check joint states, force feedback, visual confirmation\n        return len(self.latest_detections) > 0\n\n    def publish_status(self, status):\n        \"\"\"Publish assembly status\"\"\"\n        msg = String()\n        msg.data = status\n        self.status_pub.publish(msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    robot = FurnitureAssemblyRobot()\n    rclpy.spin(robot)\n    robot.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"What's Happening"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Node subscribes to assembly commands (language input)"}),"\n",(0,i.jsx)(n.li,{children:"Subscribes to perception pipeline outputs (detected parts)"}),"\n",(0,i.jsx)(n.li,{children:"Subscribes to camera images (for additional vision queries)"}),"\n",(0,i.jsx)(n.li,{children:"LLM decomposes task into steps"}),"\n",(0,i.jsxs)(n.li,{children:["For each step, executes full VLA pipeline:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand action"}),"\n",(0,i.jsx)(n.li,{children:"Perceive relevant objects"}),"\n",(0,i.jsx)(n.li,{children:"Ground language to vision"}),"\n",(0,i.jsx)(n.li,{children:"Plan motion"}),"\n",(0,i.jsx)(n.li,{children:"Execute and verify"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Publishes status updates throughout"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-output",children:"Expected Output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[furniture_assembly_robot] Furniture Assembly Robot initialized\n[furniture_assembly_robot] \ud83e\udd16 Assembly Command: Assemble the IKEA bookshelf\n[furniture_assembly_robot] \ud83d\udccb Generated 8 steps\n\n[furniture_assembly_robot] Step 1/8: pick_left_panel\n[isaac/perception] Detected: panel_left at pose (0.5, -0.2, 0.8)\n[furniture_assembly_robot] \u2713 Step 1 complete\n[furniture_assembly_robot] Step 2/8: position_vertically\n\n[furniture_assembly_robot] Step 3/8: place_on_base\n[gazebo] Physics simulation: checking stability\n[furniture_assembly_robot] \u2713 Step 3 complete\n\n... (steps 4-7) ...\n\n[furniture_assembly_robot] Step 8/8: verify_stability\n[furniture_assembly_robot] \u2713 Assembly complete!\n[furniture_assembly_robot] Furniture ready for use\n"})}),"\n",(0,i.jsx)(n.h2,{id:"why-this-capstone-matters",children:"Why This Capstone Matters"}),"\n",(0,i.jsx)(n.p,{children:"This project demonstrates:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Complete System Design"}),": All 4 modules working together seamlessly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-World Problem"}),": Furniture assembly is a genuine industry need"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Same architecture works for many assembly tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Error recovery and verification at each step"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generalization"}),": Language understanding enables new tasks without reprogramming"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"what-youve-achieved",children:"What You've Achieved"}),"\n",(0,i.jsx)(n.p,{children:"By completing all 10 chapters, you can now:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Skill"}),(0,i.jsx)(n.th,{children:"Capability"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ROS 2"})}),(0,i.jsx)(n.td,{children:"Build distributed robotic systems with pub/sub and services"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Simulation"})}),(0,i.jsx)(n.td,{children:"Validate behavior in Gazebo before hardware"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Perception"})}),(0,i.jsx)(n.td,{children:"Deploy GPU-accelerated AI for real-time vision"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Language"})}),(0,i.jsx)(n.td,{children:"Understand natural language commands via LLMs"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Autonomy"})}),(0,i.jsx)(n.td,{children:"Build closed-loop systems with perception and feedback"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Integration"})}),(0,i.jsx)(n.td,{children:"Combine all components into unified systems"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps-beyond-this-book",children:"Next Steps (Beyond This Book)"}),"\n",(0,i.jsx)(n.h3,{id:"for-practitioners",children:"For Practitioners"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Deploy on real hardware (Boston Dynamics Atlas, Tesla Optimus, etc.)"}),"\n",(0,i.jsx)(n.li,{children:"Collect real-world data for fine-tuning perception models"}),"\n",(0,i.jsx)(n.li,{children:"Implement hardware-specific optimizations"}),"\n",(0,i.jsx)(n.li,{children:"Build internal tools for your use case"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"for-researchers",children:"For Researchers"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improve language grounding through multimodal learning"}),"\n",(0,i.jsx)(n.li,{children:"Develop better manipulation planning algorithms"}),"\n",(0,i.jsx)(n.li,{children:"Study human-robot interaction in assembly tasks"}),"\n",(0,i.jsx)(n.li,{children:"Publish findings in robotics venues"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"for-educators",children:"For Educators"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Extend with more complex tasks (electronics assembly, surgical tasks)"}),"\n",(0,i.jsx)(n.li,{children:"Add reinforcement learning for policy optimization"}),"\n",(0,i.jsx)(n.li,{children:"Implement digital twin for hybrid simulation/hardware"}),"\n",(0,i.jsx)(n.li,{children:"Build community around shared benchmark tasks"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this capstone, you learned:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Integration"}),": How all 4 modules work together"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Decomposition"}),": Breaking complex problems into solvable steps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feedback Loops"}),": Verifying and adapting during execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Recovery"}),": Handling real-world failures gracefully"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Production Architecture"}),": Patterns for deployable robotic systems"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Insight"}),": Modern robotics isn't about individual algorithms\u2014it's about integration, feedback, and adaptation. This capstone shows how to build such systems."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["NVIDIA. (2024). ",(0,i.jsx)(n.em,{children:"Autonomous Manipulation with Vision-Language Models"}),". Retrieved from ",(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/blog/chatgpt-robotics",children:"https://developer.nvidia.com/blog/chatgpt-robotics"})]}),"\n",(0,i.jsxs)(n.li,{children:["Google DeepMind. (2024). ",(0,i.jsx)(n.em,{children:"Robotics Transformer (RT-2)"}),". Retrieved from ",(0,i.jsx)(n.a,{href:"https://www.deepmind.com/blog/robotics-transformer",children:"https://www.deepmind.com/blog/robotics-transformer"})]}),"\n",(0,i.jsxs)(n.li,{children:["OpenAI. (2024). ",(0,i.jsx)(n.em,{children:"Vision Capabilities of GPT-4V"}),". Retrieved from ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/vision",children:"https://platform.openai.com/docs/guides/vision"})]}),"\n",(0,i.jsxs)(n.li,{children:["ROS 2 Humble Documentation. (2024). Retrieved from ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Gazebo Physics Simulation. (2024). Retrieved from ",(0,i.jsx)(n.a,{href:"https://gazebosim.org/",children:"https://gazebosim.org/"})]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Congratulations!"})," You've completed a comprehensive robotics education program covering control, simulation, perception, and autonomy. You're ready to build real autonomous systems. \ud83d\ude80"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:function(e,n,t){t.d(n,{R:function(){return o},x:function(){return l}});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);